# -*- coding: utf-8 -*-
"""langchain-retriever.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1scae63K6qiqoY4JsabnsTp0aqKqQXobH
"""

import os
os.environ["GOOGLE_API_KEY"] = "API_KEY"

!pip install langchain chromadb faiss-cpu openai tiktoken langchain_google_genai  langchain-community wikipedia langchain_huggingface

!pip install sentence-transformers

"""**Wikipedia Retriever**"""

from langchain_community.retrievers import WikipediaRetriever

#Initialize the retriever (optional: set language and top_k)
retriever = WikipediaRetriever(top_k_results=2, lang="en")

#Define your query
query = "the geopolitical history of India and pakistan from the perspective of a chinese"

# Get relevant wikipedia document
docs = retriever.invoke(query) #retriever ek runnable hai

docs

#Print retrieved content
for i, doc in enumerate(docs):
  print(f"\n--- Result {i+1} ---")
  print(f"Content:\n{doc.page_content}...") #truncate for display

"""**Vector Store Retriever**

isme ham vector store ko hi retriever ka kaam karwayenge ( vector store ka retriever basic retrieval method upyog karta hai , advance ke liye retrievers ka upyoga karna hota hai

"""

from langchain_community.vectorstores import Chroma
# from langchain_google_genai import GoogleGenerativeAIEmbeddings #ye vaise bhi kaam nhi karta isliye sentence transformer upyog karunga
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# Step:1 Your source documents
documents = [
    Document(page_content="LangChain helps developers build LLM applications easily."),
    Document(page_content="Chroma is a vector database optimized for LLM-based search."),
    Document(page_content="Embeddings convert text into high-dimensional vectors."),
    Document(page_content="OpenAI provides powerful embedding models.")
]

# Step:2 Initialize embedding model
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Step:3 Create chroma vector store in memory
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embedding_model,
    collection_name="my_collection"

)

# Step:4 Convert vectorstore into a retriever
retriever = vectorstore.as_retriever(search_kwargs={"k":2}) #default "search_type" is "similarity"

query = "What is chroma used for?"
results = retriever.invoke(query)

for i, doc in enumerate(results):
  print(f"\n--- Result {i+1}---")
  print(doc.page_content)

results = vectorstore.similarity_search(query,k=2)

for i,ḍoc in enumerate(results):
  print(f"\n ---Result{i+1}---")
  print(doc.page_content)

"""**MMR**"""

# Sample documents
docs = [
    Document(page_content="LangChain makes it easy to work with LLMs."),
    Document(page_content="LangChain is used to build LLM based applications."),
    Document(page_content="Chroma is used to store and search document embeddings."),
    Document(page_content="Embeddings are vector representations of text."),
    Document(page_content="MMR helps you get diverse results when doing similarity search."),
    Document(page_content="LangChain supports Chroma, FAISS, Pinecone, and more."),
]

from langchain_community.vectorstores import FAISS

# Initialize embeddings
# pehleśe initialize kiya hua sentence transformer upyog karunga

# Step:2 Create the FAISS vector store from documents
vectorstore1 = FAISS.from_documents(
    documents=docs,
    embedding=embedding_model
)

#Enable MMR in the retriever
retriever1 = vectorstore1.as_retriever(
    search_type="mmr",    # <== This enables MMR
    search_kwargs={"k":3,"lambda_mult":1}  # k = top results, lambda_mult = relevance-diversity balance
)

# MMR se diversified outputs nikal sakte hai , jaise agar 2 ek jaise jawab hai toh ek hi output me dikhayega... dono nhi dikhayega set karne pe (similarity search pe ek jaise hi jawab aate hai)
# MMR me "search_kwargs" me "lambda_mult" me value 1 rakhne pe same jawab aate hai similarity seach jaise , aur 0 value pe ekdam diversify aate hai saare jawab ,ṭoh hamne apne se sahi value chunni hoti hai dono ke bich

query = "What is langchain?"
results = retriever1.invoke(query)

for i, doc in enumerate(results):
  print(f"\n ---Result{i+1}---")
  print(doc.page_content)

results = vectorstore1.similarity_search(query,k=2)

for i, doc in enumerate(results):
  print(f"\n ---Result{i+1}---")
  print(doc.page_content)

"""**Multiquery Retriever**

Here i can see difference between similarity and multiquery retriever to understand it better .

Multiquery retriever sabse pehle "query" ko llm ke paas bhejta hai aur llm uss ek query se kai query banata hai fir jitni query bani vo documents se jawab nikalti hai apne sawal ke anusaar , duplicate jawabo ko delete karti hai. Isme agar puchi gayi query confusing hai toh bhi jawab accha aata hai kyunki llm usse kai simple query bana deta hai
"""

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.retrievers.multi_query import MultiQueryRetriever

# Relevant health & wellness documents

all_docs =  [
    Document(page_content="Regular walking boosts heart health and can reduce symptoms of depression.", metadata={"source": "H1"}),
    Document(page_content="Consuming leafy greens and fruits helps detox the body and improve longevity.", metadata={"source": "H2"}),
    Document(page_content="Deep sleep is crucial for cellular repair and emotional regulation.", metadata={"source": "H3"}),
    Document(page_content="Mindfulness and controlled breathing lower cortisol and improve mental clarity.", metadata={"source": "H4"}),
    Document(page_content="Drinking sufficient water throughout the day helps maintain metabolism and energy.", metadata={"source": "H5"}),
    Document(page_content="The solar energy system in modern homes helps balance electricity demand.", metadata={"source": "I1"}),
    Document(page_content="Python balances readability with power, making it a popular system design language.", metadata={"source": "I2"}),
    Document(page_content="Photosynthesis enables plants to produce energy by converting sunlight.", metadata={"source": "I3"}),
    Document(page_content="The 2022 FIFA World Cup was held in Qatar and drew global energy and excitement.", metadata={"source": "I4"}),
    Document(page_content="Black holes bend spacetime and store immense gravitational energy.", metadata={"source": "I5"}),
]

# Initialize embedding
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create FAISS vector store
vectorstore = FAISS.from_documents(documents=all_docs, embedding=embedding_model)

# Create Retrievers
similarity_retriever = vectorstore.as_retriever(search_type="similarity",search_kwargs={"k":5})

multiquery_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    llm=ChatGoogleGenerativeAI(model="gemini-2.5-flash")
)

query = "How to improve energy levels and maintain balance (only 5 points)?"

# Retrieve Results
similarity_results = similarity_retriever.invoke(query)
multiquery_results = multiquery_retriever.invoke(query)

for i, doc in enumerate(similarity_results):
  print(f"\n ---Results{i+1}---")
  print(doc.page_content)

print("*"*150)

for i, doc in enumerate(multiquery_results):
  print(f"\n ---Results{i+1}---")
  print(doc.page_content)

"""**ContextualCompressionRetriever**"""

from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_core.documents import Document

# Creating hard data
this_docs = [
    Document(page_content=(
        """The Grand Canyon is one of the most visited natural wonders in the world.
        Photosynthesis is the process by which green plants convert sunlight into energy.
        Millions of tourists travel to see it every year. The rocks date back millions of years."""
    ), metadata={"source": "Doc1"}),

    Document(page_content=(
        """In medieval Europe, castles were built primarily for defense.
        The chlorophyll in plant cells captures sunlight during photosynthesis.
        Knights wore armor made of metal. Siege weapons were often used to breach castle walls."""
    ), metadata={"source": "Doc2"}),

    Document(page_content=(
        """Basketball was invented by Dr. James Naismith in the late 19th century.
        It was originally played with a soccer ball and peach baskets. NBA is now a global league."""
    ), metadata={"source": "Doc3"}),

    Document(page_content=(
        """The history of cinema began in the late 1800s. Silent films were the earliest form.
        Thomas Edison was among the pioneers. Photosynthesis does not occur in animal cells.
        Modern filmmaking involves complex CGI and sound design."""
    ), metadata={"source": "Doc4"})
]

# Create a FAISSvector store from the documents
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(this_docs, embedding_model)

base_retriever = vectorstore.as_retriever(search_kwargs={"k":5})

# Set up compressor using an LLM
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
compressor = LLMChainExtractor.from_llm(llm)

# Create the contextual compression retriever
compression_retriever = ContextualCompressionRetriever(
    base_retriever=base_retriever,
    base_compressor=compressor
)

# Query the retriever
query = "What is photosynthesis"
compressed_results = compression_retriever.invoke(query)

for i, doc in enumerate(compressed_results):
  print(f"\n ---Result{i+1}---")
  print(doc.page_content)

